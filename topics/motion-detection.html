<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Topic - Motion Detection using Background Subtraction</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="topic-styles.css">
    <link rel="stylesheet" href="motion-detection-styles.css">
    <script src="../projects-data.js"></script>
</head>
<body>
    <div class="notebook">
        <div class="notebook-page">
            <a href="../index.html" class="back-link">‚Üê Back to Topics</a>
            
            <div class="topic-header" id="topic-header">
                <!-- Header content will be loaded dynamically -->
            </div>

            <div class="handwritten">

                <section class="section">
                    <h2>What I Did</h2>
                    <div class="content">
                        <p>I wanted to see how different algorithms detect motion in videos. I tested five OpenCV background subtraction methods (GMG, MOG, MOG2, KNN, CNT) and built my own temporal median filter. I tried them on car traffic and beach scenes to see what works best.</p>
                    </div>
                </section>

                <section class="section">
                    <h2>How It Works</h2>
                    <div class="content">
                        <h3>OpenCV Background Subtractors</h3>
                        <p>I tested five built-in algorithms:</p>
                        <ul>
                            <li><strong>GMG</strong> - Gaussian Mixture-based, needs some frames to warm up</li>
                            <li><strong>MOG</strong> - Mixture of Gaussians</li>
                            <li><strong>MOG2</strong> - Improved version with shadow detection (worked best for me)</li>
                            <li><strong>KNN</strong> - K-Nearest Neighbors approach</li>
                            <li><strong>CNT</strong> - Counting-based, super fast</li>
                        </ul>

                        <h3>My Temporal Median Filter</h3>
                        <p>I also built a simple approach from scratch:</p>
                        <ol>
                            <li>Grab 25 random frames from the video</li>
                            <li>Calculate the median to create a background model</li>
                            <li>Compare each frame to this background</li>
                            <li>Use Otsu's thresholding to find moving objects</li>
                        </ol>

                        <h3>Cleaning Up Results</h3>
                        <p>Mrphological operations to make the detections cleaner:</p>
                        <ul>
                            <li><strong>Dilation</strong> - fills small gaps</li>
                            <li><strong>Opening</strong> - removes noise</li>
                            <li><strong>Closing</strong> - fills holes in detected objects</li>
                        </ul>
                    </div>
                </section>

                <section class="section">
                    <h2>The Code</h2>
                    <div class="content">
                        <h3>Implementation</h3>
                        <div class="code-files">
                            <div class="code-file">
                                <div class="code-header" onclick="toggleCode('bg_subtractor')">
                                    <h4>1. Background Subtraction Algorithms (other-bg-substractor-algorithms.py)</h4>
                                    <button class="copy-button" onclick="copyCode('bg_subtractor')">Copy</button>
                                </div>
                                <div class="code-content" id="bg_subtractor">
                                    <pre class="code-block"><code>import sys
from random import randint

import cv2
import numpy as np

TEXT_COLOR = (randint(0, 255), randint(0, 255), randint(0, 255))
BORDER_COLOR = (randint(0, 255), randint(0, 255), randint(0, 255))
FONT = cv2.FONT_HERSHEY_SIMPLEX
VIDEO_SOURCE = "videos-materials/input/birds.mp4"

BGS_TYPES = ['GMG', 'MOG2', 'MOG', 'KNN', 'CNT']


def get_kernel(KERNEL_TYPE):
    if KERNEL_TYPE == 'dilation':
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    if KERNEL_TYPE == 'opening':
        kernel = np.ones((3, 3), np.uint8)
    if KERNEL_TYPE == 'closing':
        kernel = np.ones((3, 3), np.uint8)
    return kernel


def get_filter(img, filter):
    if filter == 'closing':
        return cv2.morphologyEx(img, cv2.MORPH_OPEN, get_kernel('closing'), iterations=2)
    if filter == 'opening':
        return cv2.morphologyEx(img, cv2.MORPH_OPEN, get_kernel('opening'), iterations=2)
    if filter == 'dilation':
        return cv2.dilate(img, get_kernel('dilation'), iterations=2)
    if filter == 'combine':
        closing = cv2.morphologyEx(img, cv2.MORPH_OPEN, get_kernel('closing'), iterations=2)
        opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, get_kernel('opening'), iterations=2)
        dilation = cv2.dilate(img, get_kernel('dilation'), iterations=2)
        return dilation


def get_bgsubtractor(BGS_TYPE):
    if BGS_TYPE == 'GMG':
        return cv2.bgsegm.createBackgroundSubtractorGMG(initializationFrames=120, decisionThreshold=0.8)
    if BGS_TYPE == 'MOG':
        return cv2.bgsegm.createBackgroundSubtractorMOG(history=200, nmixtures=5, backgroundRatio=0.7, noiseSigma=0)
    if BGS_TYPE == 'MOG2':
        return cv2.createBackgroundSubtractorMOG2(history=500, detectShadows=True, varThreshold=100)
    if BGS_TYPE == 'KNN':
        return cv2.createBackgroundSubtractorKNN(history=500, dist2Threshold=400, detectShadows=True)
    if BGS_TYPE == 'CNT':
        return cv2.bgsegm.createBackgroundSubtractorCNT(minPixelStability=15, useHistory=True,
                                                        maxPixelStability=15 * 60, isParallel=True)
    print('Invalid detector')
    sys.exit()


cap = cv2.VideoCapture(VIDEO_SOURCE)

# MOG2 is the best for the given video Cars
BGS_TYPE = BGS_TYPES[2]
bg_subtractor = get_bgsubtractor(BGS_TYPE)


def main():
    while cap.isOpened():
        ok, frame = cap.read()

        if not ok:
            print('Error reading video file')
            break

        frame = cv2.resize(frame, (0, 0), fx=0.2, fy=0.2)
        bg_mask = bg_subtractor.apply(frame)
        fg_mask = get_filter(bg_mask, 'dilation')
        fg_mask_closing = get_filter(fg_mask, 'closing')
        fg_mask_opening = get_filter(fg_mask, 'opening')
        fg_mask_combine = get_filter(fg_mask, 'combine')

        res = cv2.bitwise_and(frame, frame, mask=fg_mask)
        res_closing = cv2.bitwise_and(frame, frame, mask=fg_mask_closing)
        res_opening = cv2.bitwise_and(frame, frame, mask=fg_mask_opening)
        res_combine = cv2.bitwise_and(frame, frame, mask=fg_mask_combine)

        cv2.putText(res_combine, 'BG subtractor: ' + BGS_TYPE, (10, 50), FONT, 1, BORDER_COLOR, 2, cv2.LINE_AA)

        cv2.imshow('Combine final', res_combine)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break


main()
cap.release()
cv2.destroyAllWindows()</code></pre>
                                </div>
                            </div>

                            <div class="code-file">
                                <div class="code-header" onclick="toggleCode('temporal_median')">
                                    <h4>2. Temporal Median Filter (temporal-median-filter.py)</h4>
                                    <button class="copy-button" onclick="copyCode('temporal_median')">Copy</button>
                                </div>
                                <div class="code-content" id="temporal_median">
                                    <pre class="code-block"><code>import numpy as np
import cv2

# Define the source and output video files
VIDEO_SOURCE = 'videos-materials/input/beach.mp4'
VIDEO_OUT = 'videos-materials/output/temporal_median_filter.avi'

# Capture video from the source file
cap = cv2.VideoCapture(VIDEO_SOURCE)
has_frame, frame = cap.read()
print(has_frame, frame.shape)

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
writer = cv2.VideoWriter(VIDEO_OUT, fourcc, 25, (frame.shape[1], frame.shape[0]), False)

# Get a random selection of frame IDs
frames_ids = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=25)
print(frames_ids)

# Capture the selected frames
frames = []
for fid in frames_ids:
    cap.set(cv2.CAP_PROP_POS_FRAMES, fid)
    has_frame, frame = cap.read()
    frames.append(frame)

# Calculate the median frame
median_frame = np.median(frames, axis=0).astype(np.uint8)
cv2.imwrite('videos-materials/output/model-median-frame.jpg', median_frame)

# Reset the video to the first frame and convert the median frame to grayscale
cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
gray_median_frame = cv2.cvtColor(median_frame, cv2.COLOR_BGR2GRAY)

while True:
    has_frame, frame = cap.read()

    if not has_frame:
        print('No frame detected')
        break

    # Convert the current frame to grayscale
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # Compute the absolute difference between the current frame and the background model
    dframe = cv2.absdiff(frame_gray, gray_median_frame)
    # Apply a threshold to get a binary image
    th, dframe = cv2.threshold(dframe, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    print(th)

    # Display the binary image
    cv2.imshow('frame', dframe)
    writer.write(dframe)

    # Exit loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release input
writer.release()
cap.release()
cv2.destroyAllWindows()</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>Results</h2>
                    <div class="content">
                        <h3>Car Traffic Scene</h3>
                        <div class="results-gallery">
                            <div class="result-item">
                                <h4>Original vs Motion Detection</h4>
                                <img src="../images/cars-comparison-video-1-min.gif" alt="Comparison showing original video and motion detection output">
                                <p>Side-by-side comparison showing how the algorithm detects moving cars</p>
                            </div>
                        </div>

                        <h3>Beach Scene with Temporal Median Filter</h3>
                        <div class="results-gallery">
                            <div class="result-item">
                                <h4>Background Model</h4>
                                <img src="../images/model-median-frame.jpg" alt="Median frame background model">
                                <p>Background model from 25 random frames</p>
                            </div>
                            <div class="result-item">
                                <h4>Input Video</h4>
                                <img src="../images/beach-input.gif" alt="Beach input video with person walking">
                                <p>Original beach video</p>
                            </div>
                            <div class="result-item">
                                <h4>Motion Detection Output</h4>
                                <img src="../images/beach-temporal-median-output.gif" alt="Temporal median filter motion detection output">
                                <p>Detected motion (slowed down for clarity)</p>
                            </div>
                        </div>

                        <div class="margin-note">
                            <strong>What I Found:</strong> MOG2 worked best overall: good balance of speed and accuracy with built-in shadow detection. CNT was fastest but less accurate. KNN handled complex scenes well but slower. The temporal median filter is simple and effective for static backgrounds.
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>Takeaways</h2>
                    <div class="content">
                        <ul>
                            <li>Different algorithms work better for different scenarios</li>
                            <li>Morphological operations make a huge difference in cleaning up detections</li>
                            <li>MOG2's shadow detection really helps with outdoor scenes</li>
                            <li>The temporal median filter is great when you have a static background</li>
                        </ul>
                    </div>
                </section>

                <section class="section">
                    <h2>Resources</h2>
                    <div class="content">
                        <ul>
                            <li>
                                <a href="https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html" target="_blank">OpenCV Background Subtraction Tutorial</a> (https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html)
                                <p>Official docs on how to use these algorithms</p>
                            </li>
                            <li>
                                <a href="https://docs.opencv.org/3.4/d7/df3/group__bgsegm.html" target="_blank">OpenCV Background Segmentation Module</a> (https://docs.opencv.org/3.4/d7/df3/group__bgsegm.html)
                                <p>API reference for GMG, MOG, and CNT</p>
                            </li>
                            <li>
                                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320313003294" target="_blank">Improved Adaptive Gaussian Mixture Model (MOG2)</a> (https://www.sciencedirect.com/science/article/abs/pii/S0031320313003294)
                                <p>Paper on MOG2 improvements</p>
                            </li>
                        </ul>
                    </div>
                </section>
            </div>
        </div>
    </div>

    <script>
        function toggleCode(id) {
            const content = document.getElementById(id);
            content.classList.toggle('active');
        }

        function copyCode(id) {
            const codeBlock = document.getElementById(id).querySelector('code');
            const textArea = document.createElement('textarea');
            textArea.value = codeBlock.textContent;
            document.body.appendChild(textArea);
            textArea.select();
            document.execCommand('copy');
            document.body.removeChild(textArea);
            
            // Show copied notification
            const button = event.target;
            const originalText = button.textContent;
            button.textContent = 'Copied!';
            setTimeout(() => {
                button.textContent = originalText;
            }, 2000);
        }

        // Load project data dynamically
        document.addEventListener('DOMContentLoaded', function() {
            const projectData = getProjectData('motion-detection');
            
            // Populate header with description before tags and date
            const headerElement = document.getElementById('topic-header');
            headerElement.innerHTML = `
                <h1>${projectData.title}</h1>
                <div class="topic-description" style="margin: 20px 0;">
                    <p>${projectData.description}</p>
                </div>
                <div class="topic-meta">
                    <p>${projectData.dateLabel}: ${projectData.date}</p>
                    <div class="topic-tags">
                        ${projectData.tags.map(tag => `<span class="tag">${tag}</span>`).join('\n                        ')}
                    </div>
                </div>
            `;
        });
    </script>
</body>
</html>
