<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Topic - Wall Segmentation in Floorplans</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="topic-styles.css">
    <link rel="stylesheet" href="wall-segmentation-styles.css">
    <script src="../projects-data.js"></script>
</head>
<body>
    <div class="notebook">
        <div class="notebook-page">
            <a href="../index.html" class="back-link">‚Üê Back to Topics</a>
            
            <div class="topic-header" id="topic-header">
                <!-- Header content will be loaded dynamically -->
            </div>

            <div class="handwritten">
                <section class="section">
                    <div class="content">
                        <p id="project-description"><!-- Description will be loaded dynamically --></p>
                    </div>
                </section>

                <section class="section">
                    <h2>The Challenge</h2>
                    <div class="content">
                        <p>Identifying walls in floorplan images might sound simple, but it's actually quite tricky. Floorplans have overlapping elements, different wall thicknesses, furniture, doors, and various architectural styles, which make automatic wall detection challenging.</p>
                        
                        <p>I used Mask R-CNN to tackle this problem. It can identify and segment objects at the pixel level, which is exactly what's needed for accurate wall detection.</p>

                        <div class="margin-note">
                            <strong>Note:</strong> Accurate wall segmentation is fundamental for subsequent tasks like room detection and 3D reconstruction.
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>What Others Have Done</h2>
                    <div class="content">
                        <p>Before diving in, I looked at what other researchers had tried:</p>
                        <table class="literature-table">
                            <thead>
                                <tr>
                                    <th style="width: 25%">Paper</th>
                                    <th style="width: 15%">Focus</th>
                                    <th style="width: 20%">Strengths</th>
                                    <th style="width: 20%">Weaknesses</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><a href="https://www.mdpi.com/2220-9964/10/12/828" target="_blank">01. Automatic Extraction of Indoor Spatial Information from Floor Plan Image</a> (https://www.mdpi.com/2220-9964/10/12/828)</td>
                                    <td>Large-Scale Complex Buildings</td>
                                    <td>
                                        <ul>
                                            <li>Vectorizes objects using CNN</li>
                                            <li>Handles 4 CVC dataset formats</li>
                                            <li>87.77% detection rate</li>
                                        </ul>
                                    </td>
                                    <td>
                                        <ul>
                                            <li>Time-consuming (5 min/225 patches)</li>
                                            <li>85.53% recognition accuracy</li>
                                        </ul>
                                    </td>
                                </tr>
                                <tr>
                                    <td><a href="https://www.mdpi.com/2076-3417/11/23/11174" target="_blank">02. Towards Robust Object Detection in Floor Plan Images</a> (https://www.mdpi.com/2076-3417/11/23/11174)</td>
                                    <td>Furniture Detection</td>
                                    <td>
                                        <ul>
                                            <li>99.8% precision</li>
                                            <li>Public dataset</li>
                                            <li>16 furniture types</li>
                                        </ul>
                                    </td>
                                    <td>
                                        <ul>
                                            <li>Synthetic data</li>
                                            <li>Computationally intensive</li>
                                        </ul>
                                    </td>
                                </tr>
                                <tr>
                                    <td><a href="https://dam-oclc.bac-lac.gc.ca/download?is_thesis=1&oclc_number=1199654049&id=5e379175-0349-4107-9c2e-aff6c6baa3f1&fileName=Cabrera-Vargas_Dany_MSc_2018.pdf" target="_blank">03. Wall Extraction and Room Detection</a> (https://dam-oclc.bac-lac.gc.ca/download?is_thesis=1&oclc_number=1199654049&id=5e379175-0349-4107-9c2e-aff6c6baa3f1&fileName=Cabrera-Vargas_Dany_MSc_2018.pdf)</td>
                                    <td>Multi-Unit Floor Plans</td>
                                    <td>
                                        <ul>
                                            <li>Combines wall/room detection</li>
                                            <li>Handles overlapping elements</li>
                                        </ul>
                                    </td>
                                    <td>
                                        <ul>
                                            <li>Complex approach</li>
                                            <li>Limited testing</li>
                                        </ul>
                                    </td>
                                </tr>
                                <tr>
                                    <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580522002217" target="_blank">04. Deep Floor Plan Recognition</a> (https://www.sciencedirect.com/science/article/abs/pii/S0926580522002217)</td>
                                    <td>Multi-Task Network</td>
                                    <td>
                                        <ul>
                                            <li>Room-boundary attention</li>
                                            <li>Handles complex shapes</li>
                                        </ul>
                                    </td>
                                    <td>
                                        <ul>
                                            <li>Requires fine-tuning</li>
                                            <li>High computational cost</li>
                                        </ul>
                                    </td>
                                </tr>
                                <tr>
                                    <td><a href="https://arxiv.org/abs/1908.11025" target="_blank">05. Parsing Line Segments</a> (https://arxiv.org/abs/1908.11025)</td>
                                    <td>Graph Neural Networks</td>
                                    <td>
                                        <ul>
                                            <li>Graph-based approach</li>
                                            <li>Captures relationships</li>
                                        </ul>
                                    </td>
                                    <td>
                                        <ul>
                                            <li>Complex implementation</li>
                                            <li>Expertise required</li>
                                        </ul>
                                    </td>
                                </tr>
                            </tbody>
                        </table>

                        <div class="key-takeaways">
                            <h3>Takeaways</h3>
                            <ul>
                                <li>CNN models work better than traditional approaches</li>
                                <li>Mask R-CNN outperforms simpler detection methods</li>
                                <li>These techniques handle complex multi-unit floorplans well</li>
                                <li>Combining multiple tasks improves overall performance</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>My Approach</h2>
                    <div class="content">
                        <h3>Why Mask R-CNN?</h3>
                        <p>I chose Mask R-CNN because it:</p>
                        <ul>
                            <li>Combines object detection and instance segmentation in one model</li>
                            <li>Provides pixel-level accuracy for wall boundaries</li>
                            <li>Handles complex floorplan layouts well</li>
                            <li>Can use pre-trained models (transfer learning)</li>
                        </ul>

                        <h3>How It Works</h3>
                        <p>Mask R-CNN is a two-stage framework with four main components:</p>

                        <div class="result-item" style="max-width: 800px; margin: 20px auto;">
                            <img src="../images/mask-r-cnn-framework-for-instance-segmentation-1.jpg" alt="Mask R-CNN Framework for Instance Segmentation">
                            <p style="text-align: center; font-style: italic;">Mask R-CNN Framework for Instance Segmentation. Source: <a href="https://viso.ai/deep-learning/mask-r-cnn/" target="_blank">viso.ai</a> (https://viso.ai/deep-learning/mask-r-cnn/)</p>
                        </div>
                        
                        <h4>Architecture Components</h4>
                        <ol>
                            <li><strong>Backbone Network</strong>
                                <ul>
                                    <li>Pre-trained CNN (e.g., ResNet-101)</li>
                                    <li>Extracts features from input images</li>
                                    <li>Generates feature maps at different scales</li>
                                </ul>
                            </li>
                            <li><strong>Region Proposal Network (RPN)</strong>
                                <ul>
                                    <li>Uses anchor boxes at multiple scales</li>
                                    <li>Generates potential object regions (RoIs)</li>
                                    <li>Predicts objectness scores</li>
                                </ul>
                            </li>
                            <li><strong>RoIAlign Layer</strong>
                                <ul>
                                    <li>Maintains precise spatial locations</li>
                                    <li>Eliminates quantization errors</li>
                                    <li>Extracts fixed-size feature maps</li>
                                </ul>
                            </li>
                            <li><strong>Mask Branch</strong>
                                <ul>
                                    <li>Uses a small Fully Convolutional Network (FCN)</li>
                                    <li>Predicts pixel-level segmentation masks</li>
                                    <li>Works in parallel with classification and bounding box regression</li>
                                </ul>
                            </li>
                        </ol>

                        <h4>Process Flow</h4>
                        <ol>
                            <li>The backbone network processes the input image and extracts features</li>
                            <li>RPN generates potential object regions using anchor boxes</li>
                            <li>RoIAlign processes these regions to extract fixed-size feature maps</li>
                            <li>The mask branch generates pixel-level segmentation masks</li>
                            <li>Final output includes class labels, bounding boxes, and segmentation masks</li>
                        </ol>
                    </div>
                </section>

                <section class="section">
                    <h2>Training Data</h2>
                    <div class="content">
                        <p>I used the <a href="https://universe.roboflow.com/floor-plan-segmentation/new_segmentation_plan" target="_blank">Floor Plan Segmentation dataset from Roboflow</a> with about 5,000 floorplan images with high-quality polygon annotations. The dataset is diverse and covers various architectural styles.</p>
                    </div>
                </section>

                <section class="section">
                    <h2>Code Implementation</h2>
                    <div class="content">
                        <h3>Key Code Files</h3>
                        <div class="code-files">
                            <div class="code-file">
                                <div class="code-header" onclick="toggleCode('floorplan_training')">
                                    <h4>1. Floorplan Training Script (floorplan_training.py)</h4>
                                    <button class="copy-button" onclick="copyCode('floorplan_training')">Copy</button>
                                </div>
                                <div class="code-content" id="floorplan_training">
                                    <pre class="code-block"><code>import os
import xml.etree.ElementTree as eT

import numpy as np
from skimage.draw import polygon

import mrcnn.config
import mrcnn.model
import mrcnn.utils


# Extract polygons
def extract_polygons(filename):
    tree = eT.parse(filename)
    root = tree.getroot()

    polygons = []
    for obj in root.findall('.//object'):
        polyp = obj.find('polygon')
        if polyp is not None:
            x_points = []
            y_points = []
            for pt in polyp.findall('.//pt'):
                x_points.append(float(pt.find('x').text))
                y_points.append(float(pt.find('y').text))

            all_points_x = np.array(x_points)
            all_points_y = np.array(y_points)
            polygons.append({
                'all_points_x': all_points_x,
                'all_points_y': all_points_y
            })

    width = int(root.find('.//size/width').text)
    height = int(root.find('.//size/height').text)

    return polygons, width, height

# Dataset class
class FloorplanDataset(mrcnn.utils.Dataset):

    def load_dataset(self, dataset_dir):
        self.add_class("dataset", 1, "Wall")

        images_dir = dataset_dir + '/images/'
        annotations_dir = dataset_dir + '/annots/'

        for i, filename in enumerate(os.listdir(images_dir)):
            image_id = filename[:-4]
            img_path = os.path.join(images_dir, filename)
            ann_path = os.path.join(annotations_dir, image_id + '.xml')

            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)

    def load_mask(self, image_id):
        info = self.image_info[image_id]
        path = info['annotation']
        polygons, w, h = self.extract_polygons(path)
        masks = np.zeros([h, w, len(polygons)], dtype=np.uint8)

        class_ids = []
        for i, poly in enumerate(polygons):
            all_points_x = [float(poly[f'x{j + 1}']) for j in range(len(poly) // 2)]
            all_points_y = [float(poly[f'y{j + 1}']) for j in range(len(poly) // 2)]

            rr, cc = polygon(all_points_y, all_points_x, (h, w))
            masks[rr, cc, i] = 1

            class_name = "Wall"
            class_id = self.class_names.index(class_name) if class_name in self.class_names else -1
            class_ids.append(class_id)

        return masks, np.array(class_ids, dtype=np.int32)

    # A helper method to extract the polygon coordinates from the annotation file
    def extract_polygons(self, filename):
        tree = eT.parse(filename)
        root = tree.getroot()

        polygons = []
        for polygon in root.findall('.//polygon'):
            poly = {}
            for i in range(1, 6):  # x1, y1, x2, y2, etc.
                x_tag = f'x{i}'
                y_tag = f'y{i}'
                x_elem = polygon.find(x_tag)
                y_elem = polygon.find(y_tag)
                if x_elem is not None and y_elem is not None:
                    poly[x_tag] = x_elem.text
                    poly[y_tag] = y_elem.text
            polygons.append(poly)

        width = int(root.find('.//size/width').text)
        height = int(root.find('.//size/height').text)
        return polygons, width, height


# Configuration class
class FloorplanConfig(mrcnn.config.Config):
    NAME = "floorplan_cfg"
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    NUM_CLASSES = 2
    STEPS_PER_EPOCH = 250

# Train model
train_dataset = FloorplanDataset()
train_dataset.load_dataset(dataset_dir='floorplan-train')
train_dataset.prepare()

validation_dataset = FloorplanDataset()
validation_dataset.load_dataset(dataset_dir='floorplan-valid')
validation_dataset.prepare()

# Model configuration
floorplan_config = FloorplanConfig()

# Build Mask R-CNN model
model = mrcnn.model.MaskRCNN(mode='training',
                             model_dir='./',
                             config=floorplan_config)

model.load_weights(filepath='mask_rcnn_coco.h5',
                   by_name=True,
                   exclude=["mrcnn_class_logits", "mrcnn_bbox_fc", "mrcnn_bbox", "mrcnn_mask"])

# Train model
model.train(train_dataset=train_dataset,
            val_dataset=validation_dataset,
            learning_rate=floorplan_config.LEARNING_RATE,
            epochs=20,
            layers='heads')

model_path = 'floorplan_mask_rcnn_trained_poly.h5'
model.keras_model.save_weights(model_path)</code></pre>
                                </div>
                            </div>

                            <div class="code-file">
                                <div class="code-header" onclick="toggleCode('floorplan_prediction')">
                                    <h4>2. Floorplan Prediction Script (floorplan_prediction.py)</h4>
                                    <button class="copy-button" onclick="copyCode('floorplan_prediction')">Copy</button>
                                </div>
                                <div class="code-content" id="floorplan_prediction">
                                    <pre class="code-block"><code>import os

import cv2

import mrcnn.config
import mrcnn.model
import mrcnn.visualize

CLASS_NAMES = ['BG', 'Wall']

class SimpleConfig(mrcnn.config.Config):
    # Give the configuration a recognizable name
    NAME = "coco_inference"
    
    # set the number of GPUs to use along with the number of images per GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    NUM_CLASSES = 2

# Initialize the Mask R-CNN model for inference and then load the weights.
# This step builds the Keras model architecture.
model = mrcnn.model.MaskRCNN(mode="inference", 
                             config=SimpleConfig(),
                             model_dir=os.getcwd())

# Load the weights into the model.
model.load_weights(filepath="floorplan_mask_rcnn_trained_poly_1500images_2epochs.h5",
                   by_name=True)

# load the input image, convert it from BGR to RGB channel
image = cv2.imread("../static/test_images/huggingface-automated-floor-plan-digitalization-04.png")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Perform a forward pass of the network to obtain the results
r = model.detect([image], verbose=0)

# Get the results for the first image.
r = r[0]

if len(r['rois']) > 0:
    print(f"Detected {len(r['rois'])} objects")

    # Visualize the detected objects.
    mrcnn.visualize.display_instances(image=image,
                                  boxes=r['rois'],
                                  masks=r['masks'],
                                  class_ids=r['class_ids'],
                                  class_names=CLASS_NAMES,
                                  scores=r['scores'])
else:
    print("No objects detected.")</code></pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>Early Experimental Results</h2>
                    <div class="content">
                        <div class="experiment-details">
                            <h3>What I Trained</h3>
                            <div class="training-notes">
                                <p>For this initial experiment, I used:</p>
                                <ul>
                                    <li>1,000 training images (out of 4,986 available)</li>
                                    <li>100 validation images</li>
                                    <li>2 epochs (planning to train for 20 eventually)</li>
                                    <li>Final loss: ~2.8 (target is ~1)</li>
                                </ul>
                                <p class="note">These results are promising considering I only used 20% of the available data and just 2 epochs.</p>
                            </div>

                            <h3>Results</h3>
                            <div class="results-grid">
                                <div class="result-item">
                                    <h4>Original Image</h4>
                                    <img src="../images/original.jpg" alt="Original floorplan image">
                                </div>
                                <div class="result-item">
                                    <h4>Bounding Boxes</h4>
                                    <img src="../images/bounding-boxes.png" alt="Floorplan with bounding boxes">
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <section class="section">
                    <h2>Additional Resources</h2>
                    <div class="content">
                        <ul>
                            <li>
                                <a href="https://github.com/cansik/architectural-floor-plan" target="_blank">Automatic analysis and simplification of architectural floor plans (Kotlin)</a> (https://github.com/cansik/architectural-floor-plan)
                                <p>A Kotlin-based tool that analyzes and simplifies architectural floor plans by detecting and extracting structural elements.</p>
                            </li>
                            <li>
                                <a href="https://github.com/dwnsingh/Object-Detection-in-Floor-Plan-Images" target="_blank">Example model training</a> (https://github.com/dwnsingh/Object-Detection-in-Floor-Plan-Images)
                                <p>A comprehensive example demonstrating how to train object detection models specifically for floor plan images.</p>
                            </li>
                            <li>
                                <a href="https://github.com/AarohiSingla/Mask-RCNN-on-Custom-Dataset-2classes-" target="_blank">Mask R-CNN implementation example</a> (https://github.com/AarohiSingla/Mask-RCNN-on-Custom-Dataset-2classes-)
                                <p>A practical implementation of Mask R-CNN for custom datasets with two classes, providing a clear example of the model's application.</p>
                            </li>
                        </ul>
                    </div>
                </section>
            </div>
        </div>
    </div>

    <script>
        function toggleCode(id) {
            const content = document.getElementById(id);
            content.classList.toggle('active');
        }

        function copyCode(id) {
            const codeBlock = document.getElementById(id).querySelector('code');
            const textArea = document.createElement('textarea');
            textArea.value = codeBlock.textContent;
            document.body.appendChild(textArea);
            textArea.select();
            document.execCommand('copy');
            document.body.removeChild(textArea);
            
            // Show copied notification
            const button = event.target;
            const originalText = button.textContent;
            button.textContent = 'Copied!';
            setTimeout(() => {
                button.textContent = originalText;
            }, 2000);
        }

        // Load project data dynamically
        document.addEventListener('DOMContentLoaded', function() {
            const projectData = getProjectData('wall-segmentation');
            
            // Populate header
            const headerElement = document.getElementById('topic-header');
            headerElement.innerHTML = `
                <h1>${projectData.title}</h1>
                <div class="topic-meta">
                    <p>Last update: ${projectData.date}</p>
                    <div class="topic-tags">
                        ${projectData.tags.map(tag => `<span class="tag">${tag}</span>`).join('\n                        ')}
                    </div>
                </div>
            `;
            
            // Populate description
            const descriptionElement = document.getElementById('project-description');
            descriptionElement.textContent = projectData.description;
        });
    </script>
</body>
</html> 